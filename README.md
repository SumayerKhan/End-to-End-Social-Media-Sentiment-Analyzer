# End-to-End Social Media Sentiment Analyzer

> Automated system for collecting, analyzing, and visualizing public sentiment about consumer electronics from Reddit discussions

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Database](https://img.shields.io/badge/Database-SQLite-blue.svg)]()
[![Status](https://img.shields.io/badge/Status-Week%203%20Complete-green.svg)]()

---

## ðŸŽ¯ Project Overview

A complete pipeline for real-time sentiment analysis of consumer electronics discussions on Reddit.

**System Components:**
- ðŸ¤– **Automated Collector** - GitHub Actions collects data every 3 hours âœ…
- ðŸ—„ï¸ **Database** - SQLite storage with sentiment scores âœ…
- ðŸ§  **Sentiment Analysis** - VADER classification system âœ…
- ðŸ”„ **Automated Pipeline** - One-command data processing âœ…
- ðŸ” **API** - REST endpoints with FastAPI *(Coming Week 4)*
- ðŸ“Š **Dashboard** - Interactive Streamlit visualization *(Coming Week 5)*

**Current Progress:** âœ… Week 3 Complete | 17,479+ posts analyzed with sentiment scores

**Future Vision:** Upgrade to RAG-based Q&A system for natural language queries about tech products

---

## ðŸ“ˆ Current Dataset Stats

- **Total Posts:** 17,479+ (and growing)
- **Subreddits Monitored:** 20 consumer electronics communities
- **Sentiment Analysis:** Complete (VADER-based classification)
- **Sentiment Distribution:**
  - Positive: 48.4%
  - Negative: 20.1%
  - Neutral: 31.5%
- **Collection Method:** Official Reddit API (PRAW) - ethical and compliant
- **Update Frequency:** Automated collection every 3 hours via GitHub Actions

---

## ðŸš€ Quick Start

### Prerequisites
- Python 3.9 or higher
- Reddit account (for API credentials)
- Git installed
- 100MB free disk space

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/SumayerKhan/End-to-End-Social-Media-Sentiment-Analyzer.git
cd End-to-End-Social-Media-Sentiment-Analyzer
```

2. **Create virtual environment**
```bash
python -m venv .venv

# Activate:
# Windows:
.venv\Scripts\activate
# Mac/Linux:
source .venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure Reddit API**
   - Go to https://www.reddit.com/prefs/apps
   - Create a new app (script type)
   - Copy the credentials

5. **Set up environment variables**
```bash
# Copy template
cp .env.example .env

# Edit .env with your credentials:
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
REDDIT_USER_AGENT=sentiment_analyzer/1.0
```

---

## ðŸ”„ Running the System

### Update Database with Latest Data (Recommended)

**Single command to update everything:**
```bash
python scripts/auto_pipeline.py
```

This automated pipeline:
1. Pulls latest JSON files from GitHub
2. Imports new posts to database
3. Runs VADER sentiment analysis on new posts
4. Displays results summary

**Output:**
- Shows number of new posts imported
- Displays sentiment analysis progress
- Provides sentiment breakdown statistics

---

### Individual Commands

**Check database statistics:**
```bash
python database/check_db.py
```

**View sentiment analysis results:**
```bash
python analyzer/show_results.py
```

**Preview random sample of posts:**
```bash
python database/preview_data.py
```

**Import JSON files manually:**
```bash
python scripts/import_from_github.py
```

**Run sentiment analysis manually:**
```bash
python analyzer/process_posts.py
```

---

## ðŸ“ Project Structure
```
sentiment-analyzer/
â”œâ”€â”€ collector/                    # Data collection (Weeks 1-2) âœ…
â”‚   â”œâ”€â”€ github_collector.py      # GitHub Actions collector
â”‚   â”œâ”€â”€ continuous_collector.py  # Alternative local collector
â”‚   â”œâ”€â”€ reddit_config.py         # Reddit API configuration
â”‚   â””â”€â”€ scheduler.py             # Collection scheduler
â”‚
â”œâ”€â”€ database/                     # Data storage âœ…
â”‚   â”œâ”€â”€ tech_sentiment.db        # SQLite database (local only)
â”‚   â”œâ”€â”€ check_db.py             # Database statistics viewer
â”‚   â””â”€â”€ preview_data.py         # Data quality checker
â”‚
â”œâ”€â”€ analyzer/                     # Sentiment analysis (Week 3) âœ…
â”‚   â”œâ”€â”€ process_posts.py        # VADER sentiment processor
â”‚   â”œâ”€â”€ show_results.py         # Results visualization
â”‚   â”œâ”€â”€ add_sentiment_columns.py # Database schema updates
â”‚   â””â”€â”€ sentiment_analyzer.py   # VADER testing utilities
â”‚
â”œâ”€â”€ scripts/                      # Automation scripts âœ…
â”‚   â”œâ”€â”€ auto_pipeline.py        # Complete pipeline orchestrator
â”‚   â””â”€â”€ import_from_github.py   # JSON to SQLite importer
â”‚
â”œâ”€â”€ data/collected/               # JSON files from GitHub Actions
â”‚   â””â”€â”€ reddit_posts_*.json     # Timestamped collections (45+ files)
â”‚
â”œâ”€â”€ .github/workflows/            # GitHub Actions automation âœ…
â”‚   â””â”€â”€ collect.yml             # Runs every 3 hours
â”‚
â”œâ”€â”€ api/                          # REST API (Week 4)
â”œâ”€â”€ dashboard/                    # Streamlit UI (Week 5)
â”œâ”€â”€ preprocessing/                # Text cleaning utilities
â”œâ”€â”€ tests/                        # Unit tests
â”‚
â”œâ”€â”€ .env                          # Secrets (NOT in repo)
â”œâ”€â”€ .env.example                  # Environment template
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # This file
â””â”€â”€ LICENSE                       # MIT License
```

---

## ðŸ› ï¸ Technical Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Language** | Python 3.13 | Core development |
| **Reddit API** | PRAW 7.8.1 | Data collection |
| **Database** | SQLite3 | Local data storage |
| **Sentiment Analysis** | VADER | Text sentiment classification âœ… |
| **Automation** | GitHub Actions | Scheduled data collection âœ… |
| **API Framework** | FastAPI | REST API endpoints *(Week 4)* |
| **Dashboard** | Streamlit | Interactive visualization *(Week 5)* |
| **Visualization** | Plotly/Recharts | Charts and graphs *(Week 5)* |

---

## ðŸŽ“ Key Features

### âœ… Implemented (Weeks 1-3)

**Data Collection:**
- [x] Multi-subreddit data collection (20 communities)
- [x] Three feed types per subreddit (new, hot, rising)
- [x] Quality filtering and spam detection
- [x] Automatic deduplication (INSERT OR IGNORE)
- [x] GitHub Actions automation (runs every 3 hours)

**Database:**
- [x] SQLite database with optimized indexes
- [x] 17,479+ posts stored
- [x] Sentiment score columns added
- [x] Data verification tools

**Sentiment Analysis:**
- [x] VADER integration and testing
- [x] Automated sentiment processing
- [x] Classification: positive/negative/neutral
- [x] Compound sentiment scores (-1 to +1)
- [x] Results visualization and statistics

**Automation:**
- [x] Complete data pipeline (import â†’ analyze â†’ report)
- [x] One-command execution
- [x] Modular, maintainable code structure
- [x] Error handling and logging

### ðŸ”„ In Progress (Week 4)
- [ ] REST API with FastAPI
- [ ] Keyword search endpoints
- [ ] Sentiment query endpoints
- [ ] API documentation with OpenAPI

### ðŸ“… Planned (Weeks 5-7)
- [ ] Interactive Streamlit dashboard
- [ ] Time-series sentiment visualization
- [ ] Filtering by subreddit, date, sentiment
- [ ] Top posts by sentiment scores
- [ ] Testing and deployment

---

## ðŸ§  Sentiment Analysis Implementation

### VADER (Valence Aware Dictionary and sEntiment Reasoner)

**Why VADER?**
- Specifically designed for social media text
- Handles slang, emojis, and internet language
- Understands emphasis (!!!, CAPS, emoticons)
- No training required
- Fast processing (~5,000 posts/minute)

**Classification Method:**
```python
# Compound score ranges:
if compound >= 0.05:  â†’ Positive
elif compound <= -0.05: â†’ Negative
else: â†’ Neutral
```

**Database Schema:**
```sql
-- Sentiment columns added to raw_posts table
ALTER TABLE raw_posts ADD COLUMN sentiment_pos REAL;      -- Positive score (0-1)
ALTER TABLE raw_posts ADD COLUMN sentiment_neg REAL;      -- Negative score (0-1)
ALTER TABLE raw_posts ADD COLUMN sentiment_neu REAL;      -- Neutral score (0-1)
ALTER TABLE raw_posts ADD COLUMN sentiment_compound REAL; -- Overall score (-1 to +1)
ALTER TABLE raw_posts ADD COLUMN sentiment_label TEXT;    -- Category label
```

**Processing Pipeline:**
1. Fetch posts without sentiment scores
2. Combine title + selftext for analysis
3. Calculate VADER scores
4. Classify into positive/negative/neutral
5. Update database with results
6. Generate statistics and reports

---

## ðŸ”„ Automated Pipeline

### System Architecture

```
GitHub Actions (Cloud - Every 3 hours)
    â†“
Collect Reddit posts â†’ Save as JSON
    â†“
Commit to GitHub repository
    â†“
[YOUR COMPUTER - Run when needed]
    â†“
python scripts/auto_pipeline.py
    â†“
    â”œâ”€â†’ Step 1: Pull latest JSON files (git pull)
    â”œâ”€â†’ Step 2: Import JSON to SQLite (skip duplicates)
    â”œâ”€â†’ Step 3: Run VADER on new posts
    â””â”€â†’ Step 4: Display results summary
    â†“
Updated local database with sentiment scores
```

### Why This Architecture?

**Advantages:**
- âœ… Automated data collection (no manual intervention)
- âœ… Version-controlled JSON files
- âœ… Local database (fast, no cloud costs)
- âœ… One command to update everything
- âœ… Modular design (easy to maintain)
- âœ… Cloud-ready (easy to deploy later)

**Files Involved:**
- `collector/github_collector.py` - Runs in GitHub Actions
- `scripts/import_from_github.py` - Imports JSON to database
- `analyzer/process_posts.py` - Runs sentiment analysis
- `scripts/auto_pipeline.py` - Orchestrates everything

---

## ðŸ“Š Development Timeline

**Week 1-2: Data Collection** âœ…
- Implemented Reddit API integration
- Built continuous collector with quality filters
- Created database schema with indexes
- Set up GitHub Actions automation
- Collected 17,479+ posts from 20 subreddits

**Week 3: Sentiment Analysis** âœ…
- Integrated VADER sentiment analyzer
- Added sentiment columns to database
- Processed all posts with sentiment scores
- Created automated pipeline script
- Built results visualization tools
- Completed documentation

**Week 4: REST API** ðŸ”„
- Build REST API with FastAPI
- Create query endpoints
- Implement filtering and search
- Generate API documentation

**Week 5-6: Dashboard** ðŸ“…
- Develop Streamlit dashboard
- Create interactive sentiment charts
- Add filtering and search UI
- Implement data export features

**Week 7: Testing & Polish** ðŸ“…
- End-to-end testing
- Performance optimization
- Final presentation preparation

---

## ðŸ“ˆ Current Results

### Sentiment Distribution (17,479 posts)

**Overall Breakdown:**
- **Positive:** 8,456 posts (48.4%)
- **Negative:** 3,512 posts (20.1%)
- **Neutral:** 5,511 posts (31.5%)

**Top Communities by Volume:**
1. r/pcmasterrace - 3,031 posts
2. r/buildapc - 2,951 posts
3. r/TechSupport - 2,565 posts
4. r/iphone - 1,708 posts
5. r/laptops - 1,276 posts

**Analysis Period:** October 19-26, 2025

---

## ðŸ—‚ï¸ Database Schema

### `raw_posts` Table
```sql
CREATE TABLE raw_posts (
    -- Original columns
    post_id TEXT PRIMARY KEY,
    subreddit TEXT NOT NULL,
    title TEXT NOT NULL,
    selftext TEXT,
    author TEXT,
    created_utc REAL NOT NULL,
    score INTEGER,
    num_comments INTEGER,
    url TEXT,
    permalink TEXT,
    collected_at REAL NOT NULL,
    
    -- Sentiment columns (Week 3)
    sentiment_pos REAL,
    sentiment_neg REAL,
    sentiment_neu REAL,
    sentiment_compound REAL,
    sentiment_label TEXT
);

-- Indexes for performance
CREATE INDEX idx_title ON raw_posts(title);
CREATE INDEX idx_created ON raw_posts(created_utc DESC);
CREATE INDEX idx_subreddit ON raw_posts(subreddit);
```

---

## ðŸ“š Subreddits Monitored

### Mobile & Wearables (6)
- r/apple, r/iphone, r/android, r/GooglePixel, r/samsung, r/GalaxyWatch

### Computers & Gaming (5)
- r/laptops, r/buildapc, r/pcgaming, r/pcmasterrace, r/battlestations

### Peripherals (3)
- r/mechanicalkeyboards, r/Monitors, r/headphones

### Gaming Handhelds (1)
- r/SteamDeck

### Smart Home (2)
- r/HomeAutomation, r/smarthome

### General & Support (3)
- r/technology, r/gadgets, r/TechSupport

---

## ðŸ”¬ Data Quality Measures

**Collection Filtering:**
- Minimum title length: 10 characters
- Excludes deleted/removed posts
- Excludes heavily downvoted content (score < -5)
- Spam detection and removal
- Duplicate prevention via database constraints

**Sentiment Analysis Quality:**
- Combined title + body text for context
- Handles empty posts gracefully
- Standard VADER thresholds (Â±0.05)
- Preserves emotional intensity (CAPS, emojis, punctuation)

**Current Quality Score:** 8/10 based on manual review

---

## ðŸš€ Future Enhancements

### Phase 1: REST API (Week 4)
- FastAPI backend with OpenAPI docs
- Query endpoints for sentiment data
- Filtering by keyword, date, subreddit, sentiment
- Aggregation endpoints for statistics

### Phase 2: Dashboard (Weeks 5-6)
- Interactive Streamlit web interface
- Time-series sentiment trends
- Subreddit comparison charts
- Top posts display
- Export functionality

### Phase 3: RAG System (Post-Project)
- **Vector Embeddings:** Semantic search with sentence-transformers
- **LLM Integration:** Claude/GPT for natural language responses
- **RAG Pipeline:** Context-aware Q&A system
- **Chat Interface:** Natural language queries

**Target Capability:**
```
User: "Should I buy the Steam Deck or wait for Steam Deck 2?"

System: [Searches 1,000+ relevant posts] â†’ [Generates contextual answer]
"Based on 347 recent discussions in r/SteamDeck, 73% of users 
recommend buying now because Steam Deck 2 hasn't been announced 
and current generation is mature with good software support..."
```

---

## ðŸ¤ Contributing

This is an academic project (CSE299), but suggestions are welcome!

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ðŸ‘¤ Author

**Sumayer Khan Sajid**  
- ID: 2221818642  
- Course: CSE299 - Junior Design Project  
- Semester: Fall 2025  
- University: North South University

---

## ðŸ™ Acknowledgments

- Reddit API (PRAW) for data access
- VADER Sentiment Analysis toolkit
- GitHub Actions for automation
- Open source community
- Course instructor and mentors

---

## ðŸ“§ Contact

- GitHub: [@SumayerKhan](https://github.com/SumayerKhan)
- Email: sumayer.cse.nsu@gmail.com

---

**âš ï¸ Note:** This project is for educational purposes. All data collection follows Reddit's Terms of Service and API usage guidelines. Database file is excluded from version control for size and privacy reasons.

---

**Last Updated:** October 26, 2025  
**Project Status:** Week 3 Complete - Sentiment Analysis Operational âœ…